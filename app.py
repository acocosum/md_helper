import streamlit as st
import os
from modules.markdown_loader import load_markdown
from modules.text_splitter import split_text
from modules.embedder import get_embedding, initialize_openai
from modules.retriever import retrieve
from modules.qa_chain_new import generate_answer

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹",
    page_icon="ğŸ“š",
    layout="wide"
)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if 'chunks' not in st.session_state:
    st.session_state.chunks = []
if 'embeddings' not in st.session_state:
    st.session_state.embeddings = []
if 'file_processed' not in st.session_state:
    st.session_state.file_processed = False
if 'openai_key' not in st.session_state:
    st.session_state.openai_key = ""
if 'api_base' not in st.session_state:
    st.session_state.api_base = ""

# æ ‡é¢˜
st.title("ğŸ“š ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹")
st.markdown("ä¸Šä¼ Markdownæ–‡ä»¶ï¼Œé€šè¿‡AIå¿«é€Ÿè·å–çŸ¥è¯†åº“å†…å®¹çš„ç­”æ¡ˆ")

# ä¾§è¾¹æ  - OpenAI APIè®¾ç½®
with st.sidebar:
    st.header("é…ç½®")
    # è·å–API keyå’Œè‡ªå®šä¹‰APIåœ°å€
    api_key = st.text_input("è¾“å…¥OpenAI API Key", type="password", value=st.session_state.openai_key)
    custom_api_base = st.text_input("è‡ªå®šä¹‰APIåœ°å€(å¯é€‰)", value=st.session_state.api_base or "https://40.chatgptsb.net/v1")

    # å¦‚æœç”¨æˆ·æä¾›äº†è‡ªå®šä¹‰APIåœ°å€ï¼Œç¡®ä¿å®ƒåªåŒ…å«åŸºç¡€URLéƒ¨åˆ†
    if custom_api_base and "/chat/completions" in custom_api_base:
        custom_api_base = custom_api_base.replace("/chat/completions", "")

    if api_key:
        st.session_state.openai_key = api_key
        st.session_state.api_base = custom_api_base
        if custom_api_base:
            initialize_openai(api_key, custom_api_base)
            st.success(f"APIå·²é…ç½®ï¼šä½¿ç”¨è‡ªå®šä¹‰APIåœ°å€ {custom_api_base}")
        else:
            initialize_openai(api_key)
            st.success("APIå·²é…ç½®ï¼šä½¿ç”¨OpenAIå®˜æ–¹API")
    
    st.markdown("---")
    st.markdown("### å‚æ•°è®¾ç½®")
    chunk_size = st.slider("æ–‡æœ¬å—å¤§å°", min_value=100, max_value=1000, value=500, step=100,
                         help="æ¯ä¸ªæ–‡æœ¬å—çš„å­—ç¬¦æ•°é‡")
    chunk_overlap = st.slider("æ–‡æœ¬å—é‡å ", min_value=0, max_value=200, value=50, step=10,
                            help="ç›¸é‚»æ–‡æœ¬å—çš„é‡å å­—ç¬¦æ•°")
    top_k = st.slider("æ£€ç´¢æ•°é‡", min_value=1, max_value=10, value=3, step=1,
                    help="æ¯æ¬¡é—®ç­”æ£€ç´¢çš„ç›¸å…³æ–‡æœ¬å—æ•°é‡")
    
    st.markdown("---")
    st.markdown("### å…³äº")
    st.info("è¿™æ˜¯ä¸€ä¸ªåŸºäºOpenAI APIçš„ä¸ªäººçŸ¥è¯†åº“åŠ©æ‰‹ï¼Œå°†æ‚¨çš„Markdownç¬”è®°è½¬åŒ–ä¸ºå¯æŸ¥è¯¢çš„çŸ¥è¯†åº“ã€‚")

# æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
uploaded_file = st.file_uploader("ä¸Šä¼ Markdownæ–‡ä»¶", type=["md"], 
                              help="é€‰æ‹©ä¸€ä¸ªMarkdownæ ¼å¼çš„æ–‡ä»¶")

# å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
if uploaded_file and not st.session_state.file_processed:
    with st.spinner("å¤„ç†æ–‡ä»¶ä¸­..."):
        # è¯»å–å¹¶è§£æMarkdownæ–‡ä»¶
        content = load_markdown(uploaded_file)
        
        # æ–‡æœ¬åˆ‡åˆ†
        chunks = split_text(content, chunk_size=chunk_size, overlap=chunk_overlap)
        st.session_state.chunks = chunks
        
        if st.session_state.openai_key:
            # è®¡ç®—Embedding
            with st.status("ç”Ÿæˆæ–‡æœ¬å‘é‡ä¸­ï¼Œè¯·ç¨å€™..."):
                embeddings = [get_embedding(chunk) for chunk in chunks]
                st.session_state.embeddings = embeddings
                st.session_state.file_processed = True
                st.success(f"æ–‡ä»¶å¤„ç†å®Œæˆï¼Œå…±åˆ‡åˆ†ä¸º{len(chunks)}ä¸ªæ–‡æœ¬å—")
        else:
            st.warning("è¯·å…ˆè®¾ç½®OpenAI API Key")

# å¦‚æœæ–‡ä»¶å·²å¤„ç†ï¼Œæ˜¾ç¤ºé—®ç­”ç•Œé¢
if st.session_state.file_processed:
    st.markdown("---")
    st.subheader("ğŸ’¬ å‘æ‚¨çš„çŸ¥è¯†åº“æé—®")
    
    # é—®é¢˜è¾“å…¥åŒºåŸŸ
    question = st.text_input("è¾“å…¥æ‚¨çš„é—®é¢˜", placeholder="ä¾‹å¦‚ï¼šè¿™ä¸ªé¡¹ç›®çš„ä¸»è¦åŠŸèƒ½æ˜¯ä»€ä¹ˆï¼Ÿ")
    
    if question:
        if len(st.session_state.chunks) > 0 and len(st.session_state.embeddings) > 0:
            with st.spinner("æ€è€ƒä¸­..."):
                # é—®é¢˜å‘é‡åŒ–
                question_embedding = get_embedding(question)
                
                # æ£€ç´¢ç›¸å…³æ–‡æœ¬å—
                relevant_indices = retrieve(question_embedding, st.session_state.embeddings, top_k=top_k)
                relevant_chunks = [st.session_state.chunks[i] for i in relevant_indices]
                
                # ç”Ÿæˆç­”æ¡ˆ
                answer = generate_answer(question, relevant_chunks)
                
                # æ˜¾ç¤ºç­”æ¡ˆ
                st.markdown("### å›ç­”")
                st.markdown(answer)
                
                # æ˜¾ç¤ºç›¸å…³å†…å®¹ï¼ˆå¯æŠ˜å ï¼‰
                with st.expander("æŸ¥çœ‹ç›¸å…³æ–‡æœ¬å—"):
                    for i, chunk in enumerate(relevant_chunks):
                        st.markdown(f"**æ–‡æœ¬å— {i+1}**")
                        st.info(chunk)
        else:
            st.error("çŸ¥è¯†åº“ä¸­æ²¡æœ‰å†…å®¹ï¼Œè¯·ä¸Šä¼ å¹¶å¤„ç†Markdownæ–‡ä»¶")
